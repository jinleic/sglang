# SGLang Attention Backends æ·±åº¦è§£æï¼šä»Pythonåˆ°GPUçŸ©é˜µä¹˜æ³•

> **ç›®æ ‡**: ç†è§£ä¸åŒattention backend (å¦‚FA3, TRT-LLM MLA, FlashInferç­‰) åœ¨SGLangä¸­çš„å®ç°å·®å¼‚ï¼Œæ·±å…¥åˆ°GPU CUDA kernelå±‚é¢çš„çŸ©é˜µä¹˜æ³•è®¡ç®—

---

## ç›®å½•

1. [Attention Backendæ¶æ„æ€»è§ˆ](#1-attention-backendæ¶æ„æ€»è§ˆ)
2. [Backendæ³¨å†Œä¸é€‰æ‹©æœºåˆ¶](#2-backendæ³¨å†Œä¸é€‰æ‹©æœºåˆ¶)
3. [FA3 Backendè¯¦ç»†å®ç°](#3-fa3-backendè¯¦ç»†å®ç°)
4. [ä¸åŒBackendè®¡ç®—å·®å¼‚å¯¹æ¯”](#4-ä¸åŒbackendè®¡ç®—å·®å¼‚å¯¹æ¯”)
5. [ä»Pythonåˆ°CUDA Kernelçš„å®Œæ•´è°ƒç”¨é“¾](#5-ä»pythonåˆ°cuda-kernelçš„å®Œæ•´è°ƒç”¨é“¾)
6. [GPUçŸ©é˜µä¹˜æ³•å±‚é¢çš„å®ç°](#6-gpuçŸ©é˜µä¹˜æ³•å±‚é¢çš„å®ç°)
7. [æ€§èƒ½å¯¹æ¯”ä¸é€‰æ‹©å»ºè®®](#7-æ€§èƒ½å¯¹æ¯”ä¸é€‰æ‹©å»ºè®®)

---

## 1. Attention Backendæ¶æ„æ€»è§ˆ

### 1.1 Backendä½“ç³»ç»“æ„

SGLangé‡‡ç”¨**æ’ä»¶åŒ–æ¶æ„**å®ç°å¤šç§attention backendsï¼š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Attention Backend Layer                        â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚   FA3    â”‚  â”‚   FA4    â”‚  â”‚ TRT-LLM  â”‚  â”‚FlashInferâ”‚  ...  â”‚
â”‚  â”‚ Backend  â”‚  â”‚ Backend  â”‚  â”‚   MLA    â”‚  â”‚ Backend  â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜       â”‚
â”‚       â”‚             â”‚              â”‚             â”‚              â”‚
â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                          â”‚                                       â”‚
â”‚                  BaseAttentionBackend                            â”‚
â”‚                  (æŠ½è±¡åŸºç±»)                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚                   â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
         â”‚  sgl_kernel    â”‚  â”‚ flashinfer  â”‚
         â”‚  (CUDA kernels)â”‚  â”‚   library   â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                 â”‚                  â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                    â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
                    â”‚   CUDA    â”‚
                    â”‚  Runtime  â”‚
                    â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
                          â”‚
                    â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
                    â”‚    GPU    â”‚
                    â”‚ Hardware  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.2 æ”¯æŒçš„Backendsåˆ—è¡¨

**æ–‡ä»¶**: `python/sglang/srt/server_args.py:88-108`

```python
ATTENTION_BACKEND_CHOICES = [
    # é€šç”¨backends
    "triton",            # Triton JITç¼–è¯‘
    "torch_native",      # PyTorchåŸç”Ÿå®ç°
    "flex_attention",    # PyTorch 2.x flex attention

    # NVIDIA GPUä¸“ç”¨
    "cutlass_mla",       # CUTLASSåº“å®ç°çš„MLA
    "fa3",               # FlashAttention 3 (Hopperä¼˜åŒ–)
    "fa4",               # FlashAttention 4 (æœ€æ–°ç‰ˆ)
    "flashinfer",        # FlashInferåº“ (é€šç”¨MHA)
    "flashmla",          # FlashInfer MLAå˜ç§
    "trtllm_mla",        # TensorRT-LLMä¼˜åŒ–çš„MLA
    "trtllm_mha",        # TensorRT-LLMä¼˜åŒ–çš„MHA
    "dual_chunk_flash_attn",  # åŒå—FlashAttention

    # AMD GPUä¸“ç”¨
    "aiter",             # AMD Instinctä¼˜åŒ–
    "wave",              # AMD Waveä¼˜åŒ–

    # å…¶ä»–å¹³å°
    "intel_amx",         # Intel AMXåŠ é€Ÿ
    "ascend",            # åä¸ºæ˜‡è…¾
]
```

### 1.3 BaseAttentionBackendæ¥å£

**æ–‡ä»¶**: `python/sglang/srt/layers/attention/base_attn_backend.py`

```python
class AttentionBackend:
    """
    Attention backendåŸºç±»

    æ‰€æœ‰backendå¿…é¡»å®ç°çš„æ ¸å¿ƒæ–¹æ³•:
    1. init_forward_metadata() - å‡†å¤‡forwardæ‰€éœ€çš„metadata
    2. forward() - æ‰§è¡Œattentionè®¡ç®—
    3. init_cuda_graph_state() - åˆå§‹åŒ–CUDA graphçŠ¶æ€ (å¯é€‰)
    """

    def init_forward_metadata(self, forward_batch: ForwardBatch):
        """
        åˆå§‹åŒ–forward metadata
        åœ¨æ¯ä¸ªforward passä¹‹å‰è°ƒç”¨ä¸€æ¬¡ï¼Œæ‰€æœ‰layerå¯å¤ç”¨
        """
        raise NotImplementedError

    def forward(
        self,
        q: torch.Tensor,
        k: torch.Tensor,
        v: torch.Tensor,
        layer: nn.Module,
        forward_batch: ForwardBatch,
    ) -> torch.Tensor:
        """
        æ‰§è¡Œattentionè®¡ç®—

        Args:
            q: Query tensor [total_tokens, num_heads, head_dim]
            k: Key tensor
            v: Value tensor
            layer: å½“å‰attention layer
            forward_batch: Forward batchä¿¡æ¯

        Returns:
            Attentionè¾“å‡º [total_tokens, num_heads, head_dim]
        """
        raise NotImplementedError
```

---

## 2. Backendæ³¨å†Œä¸é€‰æ‹©æœºåˆ¶

### 2.1 Backendæ³¨å†Œè¡¨

**æ–‡ä»¶**: `python/sglang/srt/layers/attention/attention_registry.py`

```python
# attention_registry.py:1-199

# Backendæ³¨å†Œè¡¨ (å…¨å±€å­—å…¸)
ATTENTION_BACKENDS = {}

def register_attention_backend(name):
    """
    Backendæ³¨å†Œè£…é¥°å™¨
    """
    def decorator(fn):
        ATTENTION_BACKENDS[name] = fn
        return fn
    return decorator

# === FA3 Backendæ³¨å†Œ ===
@register_attention_backend("fa3")
def create_flashattention_v3_backend(runner):
    """
    åˆ›å»ºFlashAttention v3 backend

    ç¡¬ä»¶è¦æ±‚:
    - SM>=80 (Ampere: A100, A30ç­‰)
    - SM<=90 (Hopper: H100, H200ç­‰)
    - ä¸æ”¯æŒMLAæ¨¡å‹ (MLAç”¨å…¶ä»–backend)
    """
    import torch

    assert (
        torch.cuda.get_device_capability()[0] == 8 and not runner.use_mla_backend
    ) or torch.cuda.get_device_capability()[0] == 9, (
        "FlashAttention v3 Backend requires SM>=80 and SM<=90. "
        "Please use `--attention-backend flashinfer`."
    )
    from sglang.srt.layers.attention.flashattention_backend import FlashAttentionBackend

    return FlashAttentionBackend(runner)

# === FA4 Backendæ³¨å†Œ ===
@register_attention_backend("fa4")
def create_flashattention_v4_backend(runner):
    """
    åˆ›å»ºFlashAttention v4 backend

    ç‰¹æ€§:
    - æ—©æœŸé˜¶æ®µï¼Œç›®å‰ä»…æ”¯æŒMLAæ¨¡å‹
    - ä½¿ç”¨fa_impl_ver=4å‚æ•°åŒºåˆ†v3
    """
    assert (
        runner.use_mla_backend
    ), "FlashAttention v4 Support is at an early stage, only MLA model supported now"
    from sglang.srt.layers.attention.flashattention_backend import FlashAttentionBackend

    return FlashAttentionBackend(runner, fa_impl_ver=4)

# === FlashInfer Backendæ³¨å†Œ ===
@register_attention_backend("flashinfer")
def create_flashinfer_backend(runner):
    import torch

    if not runner.use_mla_backend:
        # æ ‡å‡†MHA
        from sglang.srt.layers.attention.flashinfer_backend import FlashInferAttnBackend

        # ä¸ºEAGLE spec decodingåˆå§‹åŒ–stream
        if runner.server_args.speculative_algorithm == "EAGLE":
            if (
                not hasattr(runner, "plan_stream_for_flashinfer")
                or not runner.plan_stream_for_flashinfer
            ):
                runner.plan_stream_for_flashinfer = torch.cuda.Stream()
        return FlashInferAttnBackend(runner)
    else:
        # MLAç‰ˆæœ¬
        from sglang.srt.layers.attention.flashinfer_mla_backend import (
            FlashInferMLAAttnBackend,
        )
        return FlashInferMLAAttnBackend(runner)

# === TRT-LLM MLA Backendæ³¨å†Œ ===
@register_attention_backend("trtllm_mla")
def create_trtllm_mla_backend(runner):
    if not runner.use_mla_backend:
        raise ValueError("trtllm_mla backend can only be used with MLA models.")
    from sglang.srt.layers.attention.trtllm_mla_backend import TRTLLMMLABackend

    return TRTLLMMLABackend(runner)

# === Triton Backendæ³¨å†Œ ===
@register_attention_backend("triton")
def create_triton_backend(runner):
    assert not runner.model_config.is_encoder_decoder, (
        "Cross attention is not supported in the triton attention backend. "
        "Please use `--attention-backend flashinfer`."
    )
    if runner.server_args.enable_double_sparsity:
        from sglang.srt.layers.attention.double_sparsity_backend import (
            DoubleSparseAttnBackend,
        )
        return DoubleSparseAttnBackend(runner)
    else:
        from sglang.srt.layers.attention.triton_backend import TritonAttnBackend
        return TritonAttnBackend(runner)
```

### 2.2 Backendé€‰æ‹©æµç¨‹

**æ–‡ä»¶**: `python/sglang/srt/model_executor/model_runner.py:1050-1150`

```python
# model_runner.py:1050-1150
def init_attention_backend(self):
    """
    åˆå§‹åŒ–attention backend

    æµç¨‹:
    1. ç¡®å®šbackendåç§° (ç”¨æˆ·æŒ‡å®šæˆ–è‡ªåŠ¨é€‰æ‹©)
    2. ä»æ³¨å†Œè¡¨è·å–backendå·¥å‚å‡½æ•°
    3. è°ƒç”¨å·¥å‚å‡½æ•°åˆ›å»ºbackendå®ä¾‹
    """
    # 1. ç¡®å®šbackendåç§°
    if self.server_args.attention_backend:
        backend_name = self.server_args.attention_backend  # ç”¨æˆ·æŒ‡å®šï¼Œå¦‚ "fa3"
    else:
        # è‡ªåŠ¨é€‰æ‹©
        backend_name = self._auto_select_attention_backend()

    # 2. ä»æ³¨å†Œè¡¨è·å–å·¥å‚å‡½æ•°
    backend_factory = ATTENTION_BACKENDS.get(backend_name)
    if backend_factory is None:
        raise ValueError(f"Unknown attention backend: {backend_name}")

    # 3. åˆ›å»ºbackendå®ä¾‹
    full_attn_backend = backend_factory(self)

    # 4. åŒ…è£… (ç”¨äºhybrid GDNæ¨¡å‹ç­‰ç‰¹æ®Šæƒ…å†µ)
    self.attn_backend = attn_backend_wrapper(self, full_attn_backend)

    logger.info(f"Attention backend: {backend_name}")

def _auto_select_attention_backend(self) -> str:
    """
    è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜backend

    é€‰æ‹©é€»è¾‘:
    - MLAæ¨¡å‹: ä¼˜å…ˆtrtllm_mla > flashmla > flashinfer
    - MHAæ¨¡å‹: ä¼˜å…ˆflashinfer > fa3 > triton
    - æ ¹æ®ç¡¬ä»¶capabilityè°ƒæ•´
    """
    if self.use_mla_backend:
        # MLAæ¨¡å‹
        if is_sm90_supported():  # H100+
            return "trtllm_mla"
        else:
            return "flashinfer"
    else:
        # MHAæ¨¡å‹
        if is_flashinfer_available():
            return "flashinfer"
        elif is_fa3_default_architecture():
            return "fa3"
        else:
            return "triton"
```

---

## 3. FA3 Backendè¯¦ç»†å®ç°

### 3.1 FlashAttentionBackendç±»å®šä¹‰

**æ–‡ä»¶**: `python/sglang/srt/layers/attention/flashattention_backend.py:282-362`

```python
# flashattention_backend.py:282-362
class FlashAttentionBackend(AttentionBackend):
    """
    FlashAttention backendå®ç°

    åŒæ—¶æ”¯æŒFA3å’ŒFA4 (é€šè¿‡fa_impl_verå‚æ•°åŒºåˆ†)

    ç‰¹æ€§:
    - Prefillå’ŒDecodeç»Ÿä¸€æ¥å£
    - CUDA Graphæ”¯æŒ (ä»…Decode)
    - Speculative decodingæ”¯æŒ
    - Sliding window attentionæ”¯æŒ
    - Paged KV cache
    """

    def __init__(
        self,
        model_runner: ModelRunner,
        skip_prefill: bool = False,
        speculative_step_id=0,
        topk=0,
        speculative_num_steps=0,
        fa_impl_ver=3,  # â† FA3=3, FA4=4
    ):
        super().__init__()

        assert not (
            model_runner.sliding_window_size is not None
            and model_runner.model_config.is_encoder_decoder
        ), "Sliding window and cross attention are not supported together"

        # Metadataå­˜å‚¨
        self.forward_metadata: FlashAttentionMetadata = None
        self.forward_metadata_spec_decode_expand: FlashAttentionMetadata = None

        # æ¨¡å‹é…ç½®
        self.max_context_len = model_runner.model_config.context_len
        self.device = model_runner.device
        self.kv_cache_dtype = model_runner.kv_cache_dtype
        self.page_size = model_runner.page_size
        self.use_mla = model_runner.model_config.attention_arch == AttentionArch.MLA

        # FAç‰ˆæœ¬ (å…³é”®!)
        self.fa_impl_ver = fa_impl_ver  # 3 or 4

        # Sliding window attention
        self.sliding_window_size = model_runner.sliding_window_size
        self.has_swa = (
            self.sliding_window_size is not None and self.sliding_window_size > -1
        )

        # ç¡®å®šæ€§æ¨ç† (num_splits=1)
        self.num_splits = (
            1 if model_runner.server_args.enable_deterministic_inference else 0
        )
```

### 3.2 Metadataåˆå§‹åŒ–

**æ–‡ä»¶**: `python/sglang/srt/layers/attention/flashattention_backend.py:364-550`

```python
# flashattention_backend.py:364-550
def init_forward_metadata(self, forward_batch: ForwardBatch):
    """
    åˆå§‹åŒ–forward metadata

    æ ¹æ®forward modeå‡†å¤‡ä¸åŒçš„metadata:
    - Decode: å•tokenç”Ÿæˆ
    - Prefill/Extend: å¤štokenå¤„ç†
    - Target Verify: Speculative decodingéªŒè¯
    """
    metadata = FlashAttentionMetadata()
    seqlens_in_batch = forward_batch.seq_lens
    batch_size = forward_batch.batch_size
    device = seqlens_in_batch.device

    if forward_batch.forward_mode.is_decode_or_idle():
        # === Decodeæ¨¡å¼ ===
        metadata.cache_seqlens_int32 = seqlens_in_batch.to(torch.int32)
        metadata.max_seq_len_k = forward_batch.seq_lens_cpu.max().item()

        # cu_seqlens_q: cumulative sequence lengths for query
        # ä¾‹å¦‚: batch_size=3 â†’ [0, 1, 2, 3]
        metadata.cu_seqlens_q = torch.arange(
            0, batch_size + 1, dtype=torch.int32, device=device
        )

        # cu_seqlens_k: cumulative sequence lengths for key
        # ä¾‹å¦‚: seqlens=[10, 20, 15] â†’ [0, 10, 30, 45]
        metadata.cu_seqlens_k = torch.nn.functional.pad(
            torch.cumsum(seqlens_in_batch, dim=0, dtype=torch.int32), (1, 0)
        )

        # page_table: KV cacheçš„pageç´¢å¼•
        # shape: [batch_size, max_seq_len_k]
        metadata.page_table = forward_batch.req_to_token_pool.req_to_token[
            forward_batch.req_pool_indices, : metadata.max_seq_len_k
        ]

    elif forward_batch.forward_mode.is_extend():
        # === Prefill/Extendæ¨¡å¼ ===
        # ç±»ä¼¼Decodeï¼Œä½†max_seq_len_qå¯èƒ½>1
        ...

    # ä¿å­˜metadataä¾›æ‰€æœ‰layerä½¿ç”¨
    self.forward_metadata = metadata
```

**FlashAttentionMetadataæ•°æ®ç»“æ„**:

```python
# flashattention_backend.py:24-69
@dataclass
class FlashAttentionMetadata:
    """
    Forward passæ‰€éœ€çš„metadata
    æ¯ä¸ªlayerçš„forwardå¯ä»¥å¤ç”¨
    """

    # Sequence lengths (å·²ç¼“å­˜çš„KVé•¿åº¦)
    cache_seqlens_int32: torch.Tensor = None  # shape: [batch_size]

    # Maximum lengths
    max_seq_len_q: int = 1         # Queryçš„æœ€å¤§é•¿åº¦
    max_seq_len_k: int = 0         # Keyçš„æœ€å¤§é•¿åº¦

    # Cumulative sequence lengths (ç”¨äºvariable-length batching)
    cu_seqlens_q: torch.Tensor = None  # shape: [batch_size + 1]
    cu_seqlens_k: torch.Tensor = None  # shape: [batch_size + 1]

    # Sliding window
    window_size: tuple = (-1, -1)   # (-1, -1) è¡¨ç¤ºæ— é™ä¸Šä¸‹æ–‡

    # Page table (Paged KV cache)
    page_table: torch.Tensor = None  # shape: [batch_size, max_num_pages]

    # Encoder metadata (for cross-attention)
    encoder_cu_seqlens_k: torch.Tensor = None
    encoder_max_seq_len_k: int = 0
    encoder_lens_int32: torch.Tensor = None
    encoder_page_table: torch.Tensor = None

    # Local attention metadata (for chunked prefill)
    @dataclass
    class LocalAttentionMetadata:
        local_query_start_loc: torch.Tensor = None
        local_seqused_k: torch.Tensor = None
        local_block_table: torch.Tensor = None
        local_max_query_len: int = 0
        local_max_seq_len: int = 0

    local_attn_metadata: Optional[LocalAttentionMetadata] = None
```

### 3.3 Forwardå‡½æ•° - æ ¸å¿ƒè®¡ç®—

**æ–‡ä»¶**: `python/sglang/srt/layers/attention/flashattention_backend.py:650-900`

```python
# flashattention_backend.py:650-900
def forward(
    self,
    q: torch.Tensor,
    k: torch.Tensor,
    v: torch.Tensor,
    layer,
    forward_batch: ForwardBatch,
) -> torch.Tensor:
    """
    æ‰§è¡Œattentionè®¡ç®—

    æµç¨‹:
    1. è·å–metadata
    2. å‡†å¤‡å‚æ•° (page_table, scalingç­‰)
    3. è°ƒç”¨flash_attn_with_kvcache() CUDA kernel
    4. è¿”å›attentionè¾“å‡º
    """
    metadata = self.forward_metadata

    # === 1. ç¡®å®šattentionç±»å‹ ===
    causal = forward_batch.forward_mode.is_extend()  # Prefillç”¨causal
    window_size = metadata.window_size

    # Softcap (Gemini 2ç­‰æ¨¡å‹)
    softcap = layer.logit_cap if hasattr(layer, "logit_cap") else 0.0

    # FP8 descale (é‡åŒ–KV cache)
    if self.kv_cache_dtype_str == "fp8_e4m3":
        k_descale, v_descale = layer.kv_descale
    else:
        k_descale = v_descale = None

    # === 2. å‡†å¤‡page table ===
    page_table = metadata.page_table
    cu_seqlens_q = metadata.cu_seqlens_q
    cache_seqlens = metadata.cache_seqlens_int32
    max_seqlen_q = metadata.max_seq_len_q
    cu_seqlens_k = metadata.cu_seqlens_k

    # === 3. è°ƒç”¨Flash Attention kernel ===
    if not self.use_mla:
        # --- æ ‡å‡†MHA ---
        assert self.fa_impl_ver in [3, 4], "Only FA3/FA4 supported"

        # è·å–KV cache buffers
        key_cache, value_cache = forward_batch.token_to_kv_pool.get_kv_buffer(
            layer.layer_id
        )
        key_cache = key_cache.view(
            -1, self.page_size, layer.tp_k_head_num, layer.head_dim
        )
        value_cache = value_cache.view(
            -1, self.page_size, layer.tp_v_head_num, layer.head_dim
        )

        # å‡†å¤‡FAç‰ˆæœ¬ç‰¹å®šçš„kwargs
        kwargs = {}
        if self.fa_impl_ver != 3:
            kwargs["ver"] = self.fa_impl_ver  # FA4éœ€è¦ver=4

        # è°ƒç”¨Flash Attention CUDA kernel
        result = flash_attn_with_kvcache(
            q=q.contiguous().view(-1, layer.tp_q_head_num, layer.head_dim),
            k_cache=key_cache,
            v_cache=value_cache,
            page_table=page_table,
            cache_seqlens=cache_seqlens,
            cu_seqlens_q=cu_seqlens_q,
            cu_seqlens_k_new=cu_seqlens_k,
            max_seqlen_q=max_seqlen_q,
            softmax_scale=layer.scaling,
            causal=causal,
            window_size=window_size,
            softcap=softcap,
            k_descale=k_descale,
            v_descale=v_descale,
            return_softmax_lse=False,
            num_splits=self.num_splits,
            **kwargs,  # â† ver=4 for FA4
        )
        o = result
    else:
        # --- MLA (Multi-head Latent Attention) ---
        # ä½¿ç”¨flash_attn_varlen_funcå¤„ç†å‹ç¼©çš„KV
        output = flash_attn_varlen_func(
            q=q.view(-1, layer.tp_q_head_num, layer.head_dim),
            k=k.view(-1, layer.tp_k_head_num, layer.head_dim).to(q.dtype),
            v=v.view(-1, layer.tp_k_head_num, layer.v_head_dim).to(q.dtype),
            cu_seqlens_q=metadata.cu_seqlens_q,
            cu_seqlens_k=...,
            max_seqlen_q=metadata.max_seq_len_q,
            max_seqlen_k=...,
            softmax_scale=layer.scaling,
            causal=False,
            ver=self.fa_impl_ver,  # â† FA3 or FA4
        )
        o = output

    # === 4. è¿”å›è¾“å‡º ===
    return o.view(-1, layer.num_q_heads * layer.head_dim)
```

---

## 4. ä¸åŒBackendè®¡ç®—å·®å¼‚å¯¹æ¯”

### 4.1 Backendç‰¹æ€§å¯¹æ¯”è¡¨

| Backend | é€‚ç”¨æ¨¡å‹ | ç¡¬ä»¶è¦æ±‚ | ç‰¹æ®Šä¼˜åŒ– | KV Cacheæ ¼å¼ | æ€§èƒ½ |
|---------|---------|---------|---------|-------------|------|
| **FA3** | MHA | SM80-90 (A100,H100) | Hopper TMA, åˆ†å—å¤„ç† | Paged | â­â­â­â­â­ |
| **FA4** | MLA | SM90+ (H100+) | æœ€æ–°ä¼˜åŒ–ï¼Œæ—©æœŸé˜¶æ®µ | Paged | â­â­â­â­â­ |
| **FlashInfer** | MHA/MLA | SM80+ | é€šç”¨ä¼˜åŒ–ï¼Œç¨³å®š | Paged | â­â­â­â­ |
| **TRT-LLM MLA** | MLA | SM80+ | TensorRTä¼˜åŒ–ï¼ŒMLAå‹ç¼© | Compressed Paged | â­â­â­â­â­ |
| **Triton** | MHA | é€šç”¨ | JITç¼–è¯‘ï¼Œçµæ´» | è¿ç»­ | â­â­â­ |
| **Torch Native** | MHA | é€šç”¨ | PyTorchåŸç”Ÿï¼Œæ…¢ | è¿ç»­ | â­â­ |

### 4.2 FA3 vs TRT-LLM MLA æ ¸å¿ƒå·®å¼‚

#### **FA3 (FlashAttention 3)**

**ä¼˜åŠ¿**:
1. **Hopperæ¶æ„ä¼˜åŒ–**: åˆ©ç”¨TMA (Tensor Memory Accelerator)
2. **é€šç”¨æ€§å¼º**: æ”¯æŒMHA, GQA, MQA
3. **æˆç†Ÿç¨³å®š**: ç»è¿‡å¹¿æ³›æµ‹è¯•

**å®ç°ç‰¹ç‚¹**:
```python
# FA3ä½¿ç”¨æ ‡å‡†çš„Q, K, V
# Shape: [batch, seq_len, num_heads, head_dim]

# Decodeé˜¶æ®µ
output = flash_attn_with_kvcache(
    q=[bs, num_heads, head_dim],         # å½“å‰query
    k_cache=[num_pages, page_size, num_kv_heads, head_dim],  # å®Œæ•´KV cache
    v_cache=[num_pages, page_size, num_kv_heads, head_dim],
    ...
)

# è®¡ç®—: O = softmax(Q @ K^T / sqrt(d)) @ V
```

**å…³é”®æ–‡ä»¶**:
- `sglang/srt/layers/attention/flashattention_backend.py`
- `sgl_kernel/flash_attn.py`
- `sgl_kernel/csrc/flash_extension.cc` (C++æ¥å£)

#### **TRT-LLM MLA (Multi-head Latent Attention)**

**ä¼˜åŠ¿**:
1. **å†…å­˜é«˜æ•ˆ**: KV cacheå‹ç¼©50-70%
2. **MLAä¸“ç”¨**: é’ˆå¯¹DeepSeek-V3ç­‰æ¨¡å‹ä¼˜åŒ–
3. **TensorRTä¼˜åŒ–**: Fused kernels

**å®ç°ç‰¹ç‚¹**:
```python
# MLAä½¿ç”¨å‹ç¼©çš„KVè¡¨ç¤º
# kv_compressed = [bs, seq_len, kv_lora_rank + rope_dim]

# Decodeé˜¶æ®µ
output = flashinfer.trtllm_decode_mla(
    q=[bs, num_heads, head_dim],
    kv_cache=[num_pages, page_size, num_kv_heads, kv_lora_rank + rope_dim],  # å‹ç¼©!
    ...
)

# è®¡ç®—:
# 1. K_full = kv_cache @ W_k_lora  (è§£å‹ç¼©)
# 2. V_full = kv_cache @ W_v_lora
# 3. O = softmax(Q @ K_full^T / sqrt(d)) @ V_full
```

**å…³é”®æ–‡ä»¶**:
- `sglang/srt/layers/attention/trtllm_mla_backend.py`
- `flashinfer` library (å¤–éƒ¨ä¾èµ–)

### 4.3 è®¡ç®—æµç¨‹å¯¹æ¯”å›¾

#### **FA3 Standard MHA Flow**

```
Input: q, k, v (å½“å‰stepçš„æ–°token)
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. KV Cache Update (In-place)            â”‚
â”‚    k_cache[page_table, new_pos] = k       â”‚
â”‚    v_cache[page_table, new_pos] = v       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. Flash Attention Kernel (CUDA)         â”‚
â”‚                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ For each head (å¹¶è¡Œ):                â”‚ â”‚
â”‚  â”‚   For each query token:             â”‚ â”‚
â”‚  â”‚     S = Q @ K^T                     â”‚ â”‚  â† Tensor Core GEMM
â”‚  â”‚     P = softmax(S / sqrt(d))        â”‚ â”‚  â† Fused softmax
â”‚  â”‚     O = P @ V                       â”‚ â”‚  â† Tensor Core GEMM
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                           â”‚
â”‚  ä¼˜åŒ–:                                    â”‚
â”‚  - Tiling: åˆ†å—åŠ è½½åˆ°shared memory       â”‚
â”‚  - Online softmax: ä¸ä¿å­˜ä¸­é—´ç»“æœS       â”‚
â”‚  - Warp-levelä¼˜åŒ–                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â†“
           Output: o
```

#### **TRT-LLM MLA Flow**

```
Input: q, kv_compressed (å‹ç¼©çš„KV)
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. KV Decompression (Fused in kernel)    â”‚
â”‚    K = kv_cache @ W_k_lora                â”‚  â† ä½ç§©åˆ†è§£
â”‚    V = kv_cache @ W_v_lora                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. Attention Computation (Fused)         â”‚
â”‚                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ S = Q @ K^T                         â”‚ â”‚  â† On-the-flyè§£å‹K
â”‚  â”‚ P = softmax(S / sqrt(d))            â”‚ â”‚
â”‚  â”‚ O = P @ V                           â”‚ â”‚  â† On-the-flyè§£å‹V
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                           â”‚
â”‚  ä¼˜åŒ–:                                    â”‚
â”‚  - Fused decompression: é¿å…æ˜¾å­˜I/O     â”‚
â”‚  - Compressed cache: èŠ‚çœ50%+ æ˜¾å­˜       â”‚
â”‚  - TensorRTä¼˜åŒ–: kernel fusion          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â†“
           Output: o
```

**å†…å­˜å ç”¨å¯¹æ¯”** (DeepSeek-V3, 1K context):
```
Standard MHA (FA3):
- kv_cache_dim = head_dim = 128
- Memory per token = 2 * num_kv_heads * 128 * sizeof(dtype)
- Example: 2 * 16 * 128 * 2 = 8KB per token

MLA (TRT-LLM):
- kv_cache_dim = kv_lora_rank + rope_dim = 512 + 64 = 576
- Memory per token = num_kv_heads * 576 * sizeof(dtype)
- Example: 16 * 576 * 2 = 18KB per token

Wait, è¿™çœ‹èµ·æ¥MLAæ›´å¤§?
å®é™…ä¸ŠMLAçš„num_kv_headsé€šå¸¸æ›´å°‘ (DeepSeek-V3åªæœ‰1ä¸ªkv_head)!
- MLAå®é™…: 1 * 576 * 2 = 1.15KB per token (èŠ‚çœ85%!)
```

---

## 5. ä»Pythonåˆ°CUDA Kernelçš„å®Œæ•´è°ƒç”¨é“¾

### 5.1 è°ƒç”¨é“¾æ€»è§ˆ

```
[Pythonå±‚]
ModelRunner.forward()
    â†“
Model.forward() (DeepSeek-V3)
    â†“
DeepseekV3Attention.forward()
    â†“
self.attn_backend.forward()  â† BackendæŠ½è±¡å±‚
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ FlashAttentionBackend.forward()         â”‚  â† FA3 Backend
â”‚ (flashattention_backend.py:650)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â†“
[Python Wrapperå±‚]
flash_attn_with_kvcache()
(sgl_kernel/flash_attn.py:37)
    â†“
torch.ops.sgl_kernel.fwd.default(...)  â† PyTorch custom op
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ [C++ Extensionå±‚]                       â”‚
â”‚ sgl_kernel/csrc/flash_extension.cc      â”‚
â”‚                                         â”‚
â”‚ TORCH_LIBRARY(sgl_kernel, m) {         â”‚
â”‚   m.def("fwd", ...);                   â”‚
â”‚   m.impl("fwd", torch::kCUDA, &fwd);   â”‚
â”‚ }                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â†“
[C++ Interfaceå±‚]
fwd() wrapper function
(è°ƒç”¨flash-attentionåº“æ¥å£)
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ [CUDA Kernelå±‚]                         â”‚
â”‚ flash-attention/hopper/...              â”‚
â”‚                                         â”‚
â”‚ __global__ void                         â”‚
â”‚ flash_fwd_kernel(...) {                 â”‚
â”‚   // Hopper-optimized CUDA code        â”‚
â”‚   // Uses TMA, async copy, etc.        â”‚
â”‚ }                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â†“
[GPU Hardware]
- Tensor Cores (FP16/BF16 GEMM)
- TMA (Tensor Memory Accelerator)
- Shared Memory
- L2 Cache
```

### 5.2 è¯¦ç»†è°ƒç”¨è·¯å¾„ (FA3)

#### **Step 1: Python Backendè°ƒç”¨**

**æ–‡ä»¶**: `python/sglang/srt/layers/attention/flashattention_backend.py:774-792`

```python
# flashattention_backend.py:774-792
result = flash_attn_with_kvcache(
    q=q.contiguous().view(-1, layer.tp_q_head_num, layer.head_dim),
    k_cache=key_cache,
    v_cache=value_cache,
    page_table=page_table,
    cache_seqlens=cache_seqlens,
    cu_seqlens_q=cu_seqlens_q,
    cu_seqlens_k_new=cu_seqlens_k,
    max_seqlen_q=max_seqlen_q,
    softmax_scale=layer.scaling,
    causal=causal,
    window_size=window_size,
    softcap=softcap,
    k_descale=k_descale,
    v_descale=v_descale,
    return_softmax_lse=False,
    num_splits=self.num_splits,
    ver=3,  # â† FA3
)
```

#### **Step 2: Python Wrapperå‡½æ•°**

**æ–‡ä»¶**: `sgl-kernel/python/sgl_kernel/flash_attn.py:37-258`

```python
# flash_attn.py:37-258
def flash_attn_with_kvcache(
    q,
    k_cache,
    v_cache,
    ...,
    ver=3,
):
    """
    Flash Attention with KV cache wrapper

    å‚æ•°æ£€æŸ¥å’Œé¢„å¤„ç†ï¼Œç„¶åè°ƒç”¨åº•å±‚CUDA kernel
    """
    # FA4åˆ†æ”¯ (ä¸åŒå®ç°)
    if ver == 4:
        assert flash_attn_varlen_func_v4 is not None, "FA4 not available"
        return flash_attn_varlen_func_v4(...)

    # FA3ä¸»è·¯å¾„
    assert k_cache.stride(-1) == 1, "k_cache must have contiguous last dimension"
    assert v_cache.stride(-1) == 1, "v_cache must have contiguous last dimension"

    # è®¾ç½®é»˜è®¤scaling
    if softmax_scale is None:
        softmax_scale = q.shape[-1] ** (-0.5)

    # ç¡®ä¿contiguous (CUDA kernelè¦æ±‚)
    q, k_cache, k, v = [maybe_contiguous(x) for x in (q, k_cache, k, v)]
    v_cache = (
        v_cache.contiguous()
        if v_cache.stride(-1) != 1 and v_cache.stride(-3) != 1
        else v_cache
    )
    ...

    # ===è°ƒç”¨PyTorch custom op===
    out, softmax_lse, *rest = torch.ops.sgl_kernel.fwd.default(
        q,                    # [total_q, num_heads, head_dim]
        k_cache,              # [num_pages, page_size, num_kv_heads, head_dim]
        v_cache,              # [num_pages, page_size, num_kv_heads, head_dim]
        k,                    # None for decode
        v,                    # None for decode
        qv,                   # None
        None,                 # out (output buffer, optional)
        cu_seqlens_q,         # [batch_size + 1]
        None,                 # cu_seqlens_k
        cu_seqlens_k_new,     # None for decode
        None,                 # seqused_q
        cache_seqlens,        # [batch_size]
        max_seqlen_q,         # int
        None,                 # max_seqlen_k
        page_table,           # [batch_size, max_pages]
        cache_batch_idx,      # None
        cache_leftpad,        # None
        rotary_cos,           # None
        rotary_sin,           # None
        rotary_seqlens,       # None
        q_descale,            # FP8 descale
        k_descale,
        v_descale,
        softmax_scale,        # 1/sqrt(head_dim)
        causal,               # bool
        window_size[0],       # left window
        window_size[1],       # right window
        softcap,              # 0.0 or positive
        rotary_interleaved,   # bool
        scheduler_metadata,   # None
        num_splits,           # 0 or positive
        pack_gqa,             # None
        sm_margin,            # 0
        sinks,                # None (learnable sinks)
    )

    return (out, softmax_lse, *rest) if return_softmax_lse else out
```

#### **Step 3: C++ Extensionæ³¨å†Œ**

**æ–‡ä»¶**: `sgl-kernel/csrc/flash_extension.cc` (ç¤ºä¾‹ï¼Œå®é™…æ–‡ä»¶å¯èƒ½ç•¥æœ‰ä¸åŒ)

```cpp
// flash_extension.cc (ç®€åŒ–ç‰ˆæœ¬)
#include <torch/extension.h>
#include <sgl_kernel_ops.h>  // Flash Attention C++ interface

// Flash Attention forward wrapper
std::tuple<torch::Tensor, torch::Tensor, ...> fwd(
    torch::Tensor q,
    torch::Tensor k_cache,
    torch::Tensor v_cache,
    ...
    double softmax_scale,
    bool causal,
    int64_t window_left,
    int64_t window_right,
    ...
) {
    // å‚æ•°è½¬æ¢å’ŒéªŒè¯
    ...

    // è°ƒç”¨å®é™…çš„CUDA kernel (from flash-attention library)
    auto result = flash_attention::mha_fwd(
        q_ptr,
        k_cache_ptr,
        v_cache_ptr,
        ...,
        static_cast<float>(softmax_scale),
        causal,
        ...
    );

    return std::make_tuple(result.out, result.softmax_lse, ...);
}

// PyTorch extensionæ³¨å†Œ
TORCH_LIBRARY(sgl_kernel, m) {
    // å®šä¹‰schema (for torch.compile)
    m.def(
        "fwd("
        "Tensor q, "
        "Tensor k_cache, "
        "Tensor v_cache, "
        "Tensor? k_new, "
        "Tensor? v_new, "
        "..., "
        "float softmax_scale, "
        "bool causal, "
        "int window_left, "
        "int window_right, "
        "..."
        ") -> (Tensor, Tensor, ...)"
    );

    // ç»‘å®šCUDAå®ç°
    m.impl("fwd", torch::kCUDA, &fwd);
}
```

#### **Step 4: CUDA Kernelå±‚ (FlashAttentionåº“)**

**æ–‡ä»¶**: `flash-attention/hopper/flash_fwd_kernel.h` (å¤–éƒ¨ä¾èµ–)

```cuda
// Simplified Flash Attention CUDA Kernel (Hopperæ¶æ„)

template<typename T, int BLOCK_SIZE, int HEAD_DIM>
__global__ void flash_fwd_kernel(
    T* q,                    // [total_q, num_heads, head_dim]
    T* k_cache,              // [num_pages, page_size, num_kv_heads, head_dim]
    T* v_cache,              // [num_pages, page_size, num_kv_heads, head_dim]
    int32_t* page_table,     // [batch_size, max_pages]
    int32_t* cu_seqlens_q,   // [batch_size + 1]
    int32_t* cache_seqlens,  // [batch_size]
    float softmax_scale,
    bool causal,
    T* out                   // [total_q, num_heads, head_dim]
) {
    // === Hopperä¼˜åŒ–ç‰¹æ€§ ===
    // 1. TMA (Tensor Memory Accelerator): å¼‚æ­¥å†…å­˜æ‹·è´
    // 2. Async pipeline: é‡å è®¡ç®—å’Œå†…å­˜ä¼ è¾“
    // 3. Warpgroup-level GEMM: æ›´å¤§çš„GEMM tile

    // Thread/Blockç´¢å¼•
    int batch_idx = blockIdx.y;
    int head_idx = blockIdx.x;
    int q_idx = cu_seqlens_q[batch_idx];
    int kv_seq_len = cache_seqlens[batch_idx];

    // Shared memoryå¸ƒå±€
    __shared__ T s_q[BLOCK_SIZE][HEAD_DIM];      // Query block
    __shared__ T s_k[BLOCK_SIZE][HEAD_DIM];      // Key block
    __shared__ T s_v[BLOCK_SIZE][HEAD_DIM];      // Value block
    __shared__ float s_scores[BLOCK_SIZE][BLOCK_SIZE];  // QK^T scores

    // ===== Tiled Computation =====

    // 1. Load Query (from global memory to shared memory)
    //    ä½¿ç”¨TMAåŠ é€Ÿ
    #pragma unroll
    for (int i = threadIdx.x; i < HEAD_DIM; i += blockDim.x) {
        s_q[threadIdx.y][i] = q[q_idx * num_heads * head_dim +
                                head_idx * head_dim + i];
    }
    __syncthreads();

    // 2. Online Softmaxåˆå§‹åŒ–
    float max_score = -INFINITY;
    float sum_exp = 0.0f;
    T acc_output[HEAD_DIM] = {0};  // ç´¯åŠ è¾“å‡º

    // 3. Iterate over K/V blocks (ä»KV cache)
    int num_kv_blocks = (kv_seq_len + BLOCK_SIZE - 1) / BLOCK_SIZE;

    for (int block_idx = 0; block_idx < num_kv_blocks; ++block_idx) {
        // 3.1 Load K block (via page table)
        int page_idx = page_table[batch_idx * max_pages +
                                   block_idx * BLOCK_SIZE / page_size];
        int offset_in_page = (block_idx * BLOCK_SIZE) % page_size;

        #pragma unroll
        for (int i = threadIdx.x; i < HEAD_DIM; i += blockDim.x) {
            s_k[threadIdx.y][i] = k_cache[page_idx * page_size * num_kv_heads * head_dim +
                                          offset_in_page * num_kv_heads * head_dim +
                                          (head_idx % num_kv_heads) * head_dim + i];
        }
        __syncthreads();

        // 3.2 Compute QK^T (using Tensor Cores)
        //     Q: [1, HEAD_DIM], K^T: [HEAD_DIM, BLOCK_SIZE]
        //     Output: [1, BLOCK_SIZE]
        mma::gemm<T, BLOCK_SIZE, HEAD_DIM, 1>(
            s_q[threadIdx.y],  // Q row
            s_k,                // K block
            s_scores[threadIdx.y]  // Output scores
        );

        // 3.3 Apply scaling
        #pragma unroll
        for (int i = 0; i < BLOCK_SIZE; ++i) {
            s_scores[threadIdx.y][i] *= softmax_scale;
        }

        // 3.4 Apply causal mask (if needed)
        if (causal) {
            int q_pos = q_idx + threadIdx.y;
            #pragma unroll
            for (int k_pos = block_idx * BLOCK_SIZE;
                 k_pos < (block_idx + 1) * BLOCK_SIZE; ++k_pos) {
                if (k_pos > q_pos) {
                    s_scores[threadIdx.y][k_pos - block_idx * BLOCK_SIZE] = -INFINITY;
                }
            }
        }

        // 3.5 Online Softmax (avoid materializing full attention matrix)
        //     https://arxiv.org/abs/2112.05682
        float block_max = -INFINITY;
        #pragma unroll
        for (int i = 0; i < BLOCK_SIZE; ++i) {
            block_max = fmaxf(block_max, s_scores[threadIdx.y][i]);
        }

        float new_max = fmaxf(max_score, block_max);
        float exp_correction = expf(max_score - new_max);

        // Renormalize previous sum_exp
        sum_exp = sum_exp * exp_correction;

        // Compute new sum_exp for this block
        float block_sum_exp = 0.0f;
        #pragma unroll
        for (int i = 0; i < BLOCK_SIZE; ++i) {
            float exp_score = expf(s_scores[threadIdx.y][i] - new_max);
            s_scores[threadIdx.y][i] = exp_score;  // Overwrite with P
            block_sum_exp += exp_score;
        }
        sum_exp += block_sum_exp;
        max_score = new_max;

        // 3.6 Load V block
        #pragma unroll
        for (int i = threadIdx.x; i < HEAD_DIM; i += blockDim.x) {
            s_v[threadIdx.y][i] = v_cache[page_idx * page_size * num_kv_heads * head_dim +
                                          offset_in_page * num_kv_heads * head_dim +
                                          (head_idx % num_kv_heads) * head_dim + i];
        }
        __syncthreads();

        // 3.7 Compute P @ V (using Tensor Cores)
        //     P: [1, BLOCK_SIZE], V: [BLOCK_SIZE, HEAD_DIM]
        //     Output: [1, HEAD_DIM]
        T block_output[HEAD_DIM];
        mma::gemm<T, 1, BLOCK_SIZE, HEAD_DIM>(
            s_scores[threadIdx.y],  // P row
            s_v,                     // V block
            block_output             // Output
        );

        // 3.8 Accumulate output (with correction for online softmax)
        #pragma unroll
        for (int i = 0; i < HEAD_DIM; ++i) {
            acc_output[i] = acc_output[i] * exp_correction + block_output[i];
        }

        __syncthreads();
    }

    // 4. Final normalization
    #pragma unroll
    for (int i = 0; i < HEAD_DIM; ++i) {
        acc_output[i] /= sum_exp;
    }

    // 5. Write output (to global memory)
    #pragma unroll
    for (int i = threadIdx.x; i < HEAD_DIM; i += blockDim.x) {
        out[q_idx * num_heads * head_dim + head_idx * head_dim + i] = acc_output[i];
    }
}
```

---

## 6. GPUçŸ©é˜µä¹˜æ³•å±‚é¢çš„å®ç°

### 6.1 Attentionä¸­çš„å…³é”®çŸ©é˜µä¹˜æ³•

Attentionè®¡ç®—åŒ…å«**ä¸¤ä¸ªä¸»è¦GEMMæ“ä½œ**:

```
1. QK^T: [batch, num_heads, seq_q, head_dim] @ [batch, num_heads, head_dim, seq_k]
         â†’ [batch, num_heads, seq_q, seq_k]

2. P@V:  [batch, num_heads, seq_q, seq_k] @ [batch, num_heads, seq_k, head_dim]
         â†’ [batch, num_heads, seq_q, head_dim]
```

### 6.2 Tensor CoreåŠ é€Ÿ

**Tensor Cores** æ˜¯NVIDIA GPU (Volta+) ä¸Šçš„ä¸“ç”¨çŸ©é˜µä¹˜æ³•å•å…ƒ:

```
ä¼ ç»ŸCUDA Core (å•ç²¾åº¦):
- 1 CUDA core/cycle = 1 FP32 FMAæ“ä½œ

Tensor Core (æ··åˆç²¾åº¦):
- 1 Tensor Core/cycle = 64 FP16 FMAæ“ä½œ (4x4x4 matrix multiply)
- H100 Tensor Core = 256 FP16 FMA/cycle (16x8x16)
```

**åœ¨FlashAttentionä¸­çš„ä½¿ç”¨**:

```cuda
// Tensor Core GEMM (WMMA API)
#include <mma.h>
using namespace nvcuda::wmma;

// Fragment declarations (å¯„å­˜å™¨ä¸­çš„çŸ©é˜µç‰‡æ®µ)
fragment<matrix_a, 16, 16, 16, half, row_major> a_frag;
fragment<matrix_b, 16, 16, 16, half, col_major> b_frag;
fragment<accumulator, 16, 16, 16, float> c_frag;

// 1. Load fragments from shared memory
load_matrix_sync(a_frag, s_q, 16);  // Q fragment
load_matrix_sync(b_frag, s_k, 16);  // K fragment

// 2. Matrix multiply-accumulate (Tensor Coreæ‰§è¡Œ)
mma_sync(c_frag, a_frag, b_frag, c_frag);

// 3. Store result back to shared memory
store_matrix_sync(s_scores, c_frag, 16, mem_row_major);
```

**æ€§èƒ½æå‡**:
- FP16 Tensor Core: ~100-300 TFLOPS (H100)
- FP32 CUDA Core: ~30-60 TFLOPS
- **åŠ é€Ÿæ¯”: 3-10x**

### 6.3 FlashAttentionçš„å†…å­˜ä¼˜åŒ–

**ä¼ ç»ŸAttentioné—®é¢˜**:
```python
# æ ‡å‡†å®ç° (ä¼šOOM)
S = Q @ K.T                    # [batch, heads, seq_q, seq_k]
P = softmax(S, dim=-1)         # [batch, heads, seq_q, seq_k]
O = P @ V                      # [batch, heads, seq_q, head_dim]

# é—®é¢˜: Så’ŒPçš„æ˜¾å­˜å ç”¨ = O(seq_q * seq_k)
# ä¾‹å¦‚: seq=4096, batch=8, heads=32
# Så’ŒPæ˜¾å­˜ = 8 * 32 * 4096 * 4096 * 4 bytes = 16 GB!
```

**FlashAttentionè§£å†³æ–¹æ¡ˆ**:

```
æ ¸å¿ƒæ€æƒ³:
1. Tiling: åˆ†å—å¤„ç†ï¼Œä¸€æ¬¡åªè®¡ç®—ä¸€å°å—
2. Online Softmax: åœ¨çº¿æ›´æ–°softmaxç»Ÿè®¡é‡ï¼Œä¸ä¿å­˜å®Œæ•´SçŸ©é˜µ
3. Recomputation: åå‘ä¼ æ’­æ—¶é‡æ–°è®¡ç®—ï¼Œä¸ä¿å­˜PçŸ©é˜µ
```

**åˆ†å—ç¤ºæ„å›¾**:

```
Q matrix: [seq_q, head_dim]
K matrix: [seq_k, head_dim]
V matrix: [seq_k, head_dim]

Split Q into Br blocks (row blocks)
Split K,V into Bc blocks (column blocks)

For each Q block Qi (size [Br, head_dim]):
    Load Qi to SRAM (shared memory)

    Initialize: max_score = -âˆ, sum_exp = 0, Oi = 0

    For each K,V block Kj, Vj (size [Bc, head_dim]):
        Load Kj, Vj to SRAM

        # Compute attention scores (åœ¨SRAM)
        Sij = Qi @ Kj.T              # [Br, Bc]
        Sij = Sij / sqrt(head_dim)

        # Online softmax update
        block_max = max(Sij)
        new_max = max(max_score, block_max)

        # Renormalize previous results
        correction = exp(max_score - new_max)
        sum_exp = sum_exp * correction
        Oi = Oi * correction

        # Compute softmax for this block
        Pij = exp(Sij - new_max)    # [Br, Bc]
        sum_exp += sum(Pij)

        # Accumulate output
        Oi += Pij @ Vj              # [Br, head_dim]

        max_score = new_max

    # Final normalization
    Oi = Oi / sum_exp

    # Write Oi to HBM (global memory)
    Store Oi
```

**å†…å­˜å ç”¨å¯¹æ¯”**:

| æ–¹æ³• | SçŸ©é˜µ | PçŸ©é˜µ | SRAMä½¿ç”¨ | æ€»æ˜¾å­˜ |
|------|-------|-------|---------|--------|
| æ ‡å‡†Attention | O(NÂ²) | O(NÂ²) | O(1) | O(NÂ²) |
| FlashAttention | - | - | O(N) | O(N) |

å…¶ä¸­N = seq_len

### 6.4 Hopperæ¶æ„ä¸“å±ä¼˜åŒ– (H100)

**TMA (Tensor Memory Accelerator)**:

```cuda
// ä¼ ç»ŸCUDAå†…å­˜æ‹·è´ (é€šè¿‡L1/L2 cache)
__shared__ float s_data[BLOCK_SIZE];
for (int i = threadIdx.x; i < BLOCK_SIZE; i += blockDim.x) {
    s_data[i] = global_data[offset + i];  // æ¯ä¸ªçº¿ç¨‹load
}
__syncthreads();

// TMAå¼‚æ­¥æ‹·è´ (Hopperç¡¬ä»¶åŠ é€Ÿ)
#include <cuda/barrier>
#include <cuda/pipeline>

__shared__ float s_data[BLOCK_SIZE];
cuda::pipeline<cuda::thread_scope_block> pipe = cuda::make_pipeline();

// å¼‚æ­¥æ‹·è´ (ç¡¬ä»¶DMA)
pipe.producer_acquire();
cuda::memcpy_async(
    s_data,              // dest (shared memory)
    global_data + offset,// src (global memory)
    BLOCK_SIZE * sizeof(float),
    pipe
);
pipe.producer_commit();

// è®¡ç®—å…¶ä»–å†…å®¹ (overlap with memory transfer)
...

// ç­‰å¾…æ‹·è´å®Œæˆ
pipe.consumer_wait();
__syncthreads();
```

**ä¼˜åŠ¿**:
- **ååé‡**: TMAååé‡ ~2-3x ä¼ ç»Ÿload
- **å»¶è¿Ÿéšè—**: å¼‚æ­¥æ‹·è´ä¸è®¡ç®—é‡å 
- **L2 cache bypass**: ç›´æ¥è®¿é—®HBM

---

## 7. æ€§èƒ½å¯¹æ¯”ä¸é€‰æ‹©å»ºè®®

### 7.1 Benchmarkç»“æœ (ç¤ºä¾‹, å®é™…æ€§èƒ½å–å†³äºç¡¬ä»¶å’Œé…ç½®)

**æµ‹è¯•ç¯å¢ƒ**: H100 80GB, Batch=8, SeqLen=2048, Hidden=5120

| Backend | Prefill (ms) | Decode (ms/token) | å†…å­˜ (GB) | ååé‡ (tokens/s) |
|---------|-------------|-------------------|-----------|-------------------|
| FA3 (MHA) | 45 | 2.1 | 12 | 380 |
| FA4 (MLA) | 40 | 1.8 | 7 | 450 |
| TRT-LLM MLA | 38 | 1.6 | 6 | 500 |
| FlashInfer | 48 | 2.3 | 12 | 350 |
| Triton | 65 | 3.5 | 15 | 230 |
| Torch Native | 120 | 8.0 | 18 | 100 |

### 7.2 Backendé€‰æ‹©å†³ç­–æ ‘

```
                    å¼€å§‹
                      â†“
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚  MLAæ¨¡å‹?    â”‚
              â””â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”˜
                 â”‚Yes     â”‚No
                 â†“        â†“
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  H100+?   â”‚  â”‚  H100+?   â”‚
         â””â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”˜  â””â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”˜
           â”‚Yes  â”‚No      â”‚Yes  â”‚No
           â†“     â†“        â†“     â†“
      TRT-LLM  FlashInfer  FA3  FlashInfer
       MLA                      (æˆ–Triton)
```

### 7.3 æ¨èé…ç½®

#### **DeepSeek-V3 (MLAæ¨¡å‹)**
```bash
# æœ€ä¼˜æ€§èƒ½ (H100)
--attention-backend trtllm_mla

# ç¨³å®šæ€§ä¼˜å…ˆ (A100/H100)
--attention-backend flashinfer

# æ—©æœŸå°è¯•æœ€æ–°ä¼˜åŒ– (H100, å®éªŒæ€§)
--attention-backend fa4
```

#### **Llama-3 (æ ‡å‡†MHA)**
```bash
# H100æ¨è
--attention-backend fa3

# A100æ¨è
--attention-backend flashinfer

# é€šç”¨ (æ‰€æœ‰GPU)
--attention-backend triton
```

### 7.4 è°ƒè¯•å’ŒProfileå»ºè®®

**æŸ¥çœ‹å®é™…ä½¿ç”¨çš„backend**:
```python
# æ—¥å¿—ä¸­ä¼šè¾“å‡º
logger.info(f"Attention backend: {backend_name}")
```

**Profile GPU kernel**:
```bash
# ä½¿ç”¨NVIDIA Nsight Compute
ncu --set full -o profile.ncu-rep \
    python -m sglang.launch_server --model ... --attention-backend fa3

# æŸ¥çœ‹æŠ¥å‘Š
ncu-ui profile.ncu-rep
```

**å¸¸è§æ€§èƒ½é—®é¢˜**:
1. **Page tableç¢ç‰‡**: ä½¿ç”¨`--max-total-tokens`é™åˆ¶KV cache
2. **Batch sizeå¤ªå°**: æ— æ³•å……åˆ†åˆ©ç”¨GPUï¼Œå¢å¤§batch
3. **Sequence lengthä¸å‡åŒ€**: å¯¼è‡´paddingæµªè´¹ï¼Œä½¿ç”¨continuous batching

---

## é™„å½•: å…³é”®æ–‡ä»¶ç´¢å¼•

### Backendæ³¨å†Œä¸ç®¡ç†
1. `python/sglang/srt/layers/attention/attention_registry.py` - Backendæ³¨å†Œè¡¨
2. `python/sglang/srt/server_args.py:88-108` - Backendé€‰é¡¹å®šä¹‰
3. `python/sglang/srt/model_executor/model_runner.py:1050-1150` - Backendåˆå§‹åŒ–

### FA3/FA4å®ç°
4. `python/sglang/srt/layers/attention/flashattention_backend.py` - FlashAttention Backendä¸»ç±»
5. `sgl-kernel/python/sgl_kernel/flash_attn.py` - Python wrapper
6. `sgl-kernel/csrc/flash_extension.cc` - C++ extension
7. External: `flash-attention` library - CUDA kernels

### TRT-LLM MLAå®ç°
8. `python/sglang/srt/layers/attention/trtllm_mla_backend.py` - TRT-LLM MLA Backend
9. External: `flashinfer` library - TRT-LLM optimized kernels

### å…¶ä»–Backends
10. `python/sglang/srt/layers/attention/flashinfer_backend.py` - FlashInfer Backend (MHA)
11. `python/sglang/srt/layers/attention/flashinfer_mla_backend.py` - FlashInfer Backend (MLA)
12. `python/sglang/srt/layers/attention/triton_backend.py` - Triton Backend
13. `python/sglang/srt/layers/attention/torch_native_backend.py` - PyTorch Native Backend

### åŸºç¡€è®¾æ–½
14. `python/sglang/srt/layers/attention/base_attn_backend.py` - AttentionBackendåŸºç±»
15. `sgl-kernel/README.md` - sgl-kernelåº“æ–‡æ¡£
16. `sgl-kernel/CMakeLists.txt` - CUDA kernelç¼–è¯‘é…ç½®

---

## æ€»ç»“

æœ¬æ–‡æ¡£è¯¦ç»†è§£æäº†SGLangä¸­ä¸åŒattention backendsçš„å®ç°ï¼Œä»Python APIåˆ°GPU CUDA kernelçš„å®Œæ•´è°ƒç”¨é“¾ï¼š

1. **æ¶æ„å±‚é¢**: æ’ä»¶åŒ–è®¾è®¡ï¼Œé€šè¿‡æ³¨å†Œè¡¨åŠ¨æ€é€‰æ‹©backend
2. **å®ç°å±‚é¢**: FA3/FA4å…±äº«FlashAttentionBackendç±»ï¼Œé€šè¿‡`fa_impl_ver`å‚æ•°åŒºåˆ†
3. **è®¡ç®—å±‚é¢**:
   - æ ‡å‡†MHA: ç›´æ¥Q@K^T@V
   - MLA: å‹ç¼©KVè¡¨ç¤ºï¼Œon-the-flyè§£å‹ç¼©
4. **ä¼˜åŒ–å±‚é¢**:
   - Tensor CoreåŠ é€ŸçŸ©é˜µä¹˜æ³• (3-10x)
   - Tiling + Online Softmaxå‡å°‘æ˜¾å­˜ (O(NÂ²) â†’ O(N))
   - Hopper TMAå¼‚æ­¥å†…å­˜ä¼ è¾“ (2-3xå¸¦å®½)

**å…³é”®æ´å¯Ÿ**:
- Backendé€‰æ‹©å¯¹æ€§èƒ½å½±å“å·¨å¤§ (å¯è¾¾5xå·®å¼‚)
- MLAæ¨¡å‹å¿…é¡»ä½¿ç”¨ä¸“ç”¨backend (trtllm_mla/flashmla)
- H100ä¸ŠFA3/TRT-LLMå¯å‘æŒ¥æœ€å¤§æ€§èƒ½
- ç†è§£ä»Pythonåˆ°CUDAçš„è°ƒç”¨é“¾æœ‰åŠ©äºè°ƒè¯•å’Œä¼˜åŒ–

å¸Œæœ›è¿™ä»½æ–‡æ¡£èƒ½å¸®åŠ©ä½ æ·±å…¥ç†è§£SGLangçš„attentionè®¡ç®—å®ç°! ğŸš€
